{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be934e6-e523-4d30-aa7a-dbd12119fb01",
   "metadata": {},
   "source": [
    "# Assignment 3: streaming analytics on text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf04691-4714-40b7-b5ea-3d14c8e7ec4c",
   "metadata": {},
   "source": [
    "## Spark setting and data import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5296fe-77da-4ecb-8484-bd5faa0c80f9",
   "metadata": {},
   "source": [
    "The task the about streaming analysis, including text analysis, building machine learning models specifically with PySpark and how to obtain data from streaming. The data collected are from a Spark setting. After duplicate removal there is about 5 thoudsand entries in the data set, which is large enough for a mahchine learning model on text analysis. The data is split for training and testing and preprocessed as what should be done normally in machine learning. Alongside, some text-analysis-specific methods are used like punctuation/stopwords removal as well as TF-IDF.\n",
    "\n",
    "The first step is to introduce saved stories collected via Spark. For sake of model performance, dataframe used is Saprk Dataframe instead of RDD or Pandas Dataframe. First of all, a PySpark environment has been set up. The saved stories is read through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e8d30c-adf5-4615-ba55-701129b84f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cbd206-6c6b-48b0-8317-e1dbe461c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b82eb1-2138-4b2c-9047-f434ea6708ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.46.203.199:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e558ca6-01ae-404d-834c-bb48a4ac0b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.46.203.199:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d92c6d8bd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ce2783-b61f-4efc-9a24-69a9631f7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read through all the subdirectories saved\n",
    "df = spark.read.json(\"C:/Users/Admin/Desktop/spark_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569975a2-e449-4f47-bdbd-e52552c492bd",
   "metadata": {},
   "source": [
    "## Preprocession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0f6e9-e53c-40f1-9985-019ea1176d26",
   "metadata": {},
   "source": [
    "Transaformations on the Spark Dataframe is executed for model training. Before transformation, duplicate rows are dropped baecause Spark cannot tell whether a data has been collected so there are overlapped entries. Then, some useless columns are dropped. For columns like user, aid, url, posted_at and domain, they are of no use because they are too unique. There are two 'titles': 'title' and 'source_title'. We only keep 'title' considering the fact that people would browse Hector News initially instead of browsing the source titles. Missing values are detected and dropped as well. As inspected, there are two kinds of missing values. One type is 'NULL' contents. Another one is '404 error' (shown as 'Page not found')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc594d7a-492e-4842-ae70-1d96c34175d7",
   "metadata": {},
   "source": [
    "### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7aee8b-2d7d-4ce8-b76c-e1efe13bccbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4396"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate rows and count\n",
    "df = df.dropDuplicates()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "effc6ba3-b6b9-4233-8042-b3edb525f97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------+-------------------+--------------------+\n",
      "|     aid|               title|                 url|frontpage|         source_text|votes|comments|           domain|     user|          posted_at|        source_title|\n",
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------+-------------------+--------------------+\n",
      "|40364744|A design for micr...|https://www.nextb...|    false|Millimeter accura...|    1|       0|nextbigfuture.com|    fanf2|2024-05-15 09:24:02|Millimeter accura...|\n",
      "|40133032|Unreal Engine 5.4...|https://dev.epicg...|    false|Unreal Engine 5.4...|    1|       0|    epicgames.com|makepanic|2024-04-23 15:27:19|Unreal Engine 5.4...|\n",
      "|40206221|Sunlight and Vita...|https://www.ncbi....|     true|Sunlight and Vita...|   43|      42|          nih.gov|      luu|2024-04-30 01:10:50|Sunlight and Vita...|\n",
      "|40360417|Bryan Caplan on A...|https://reason.co...|    false|Q&A: Bryan Caplan...|    1|       0|       reason.com| jseliger|2024-05-14 21:35:02|Q&A: Bryan Caplan on|\n",
      "|40269847|Probing single el...|https://www.natur...|    false|Probing single el...|    1|       0|       nature.com|  gnabgib|2024-05-06 00:11:55|Probing single el...|\n",
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"aid\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"frontpage\", StringType(), True),\n",
    "    StructField(\"source_text\", StringType(), True),\n",
    "        StructField(\"votes\", IntegerType(), True),\n",
    "    StructField(\"comments\", IntegerType(), True),\n",
    "\n",
    "        StructField(\"domain\", StringType(), True),\n",
    "        StructField(\"user\", StringType(), True),\n",
    "    StructField(\"posted_at\", StringType(), True),\n",
    "        StructField(\"source_title\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Parse the JSON strings using the defined schema\n",
    "df = df.withColumn(\"parsed_value\", from_json(col(\"value\"), schema)).select(\"parsed_value.*\")\n",
    "\n",
    "# Show the DataFrame with all columns\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eeabff8-c498-492f-ae73-857f51223c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+--------+\n",
      "|               title|frontpage|         source_text|votes|comments|\n",
      "+--------------------+---------+--------------------+-----+--------+\n",
      "|A design for micr...|    false|Millimeter accura...|    1|       0|\n",
      "|Unreal Engine 5.4...|    false|Unreal Engine 5.4...|    1|       0|\n",
      "|Sunlight and Vita...|     true|Sunlight and Vita...|   43|      42|\n",
      "|Bryan Caplan on A...|    false|Q&A: Bryan Caplan...|    1|       0|\n",
      "|Probing single el...|    false|Probing single el...|    1|       0|\n",
      "+--------------------+---------+--------------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop some columns which wwould never be used\n",
    "df = df.drop('aid','source_title','posted_at','user','url','domain')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc994cd-008f-427f-9dde-217d131f25fd",
   "metadata": {},
   "source": [
    "### Removing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "084d3f29-76b8-4de9-8202-da34a73e974e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4340"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values check: 2 types could be viewed as missing values, then count\n",
    "# Type 1: Page not found\n",
    "# Type 2: NULL\n",
    "df = df.where(df.source_text != 'Page not found')\n",
    "df.dropna()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8af23-bf6a-4d9c-9f40-a038b5363d5b",
   "metadata": {},
   "source": [
    "### Encoding the label column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea435c07-890e-4533-8923-c3f6b6facd6e",
   "metadata": {},
   "source": [
    "The label variable 'frontpage' is in string type. To make predictions, it is converted as a dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa49ccc-7bb6-4130-9e43-fd01bbcea521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+--------+\n",
      "|               title|frontpage|         source_text|votes|comments|\n",
      "+--------------------+---------+--------------------+-----+--------+\n",
      "|A design for micr...|        0|Millimeter accura...|    1|       0|\n",
      "|Unreal Engine 5.4...|        0|Unreal Engine 5.4...|    1|       0|\n",
      "|Sunlight and Vita...|        1|Sunlight and Vita...|   43|      42|\n",
      "|Bryan Caplan on A...|        0|Q&A: Bryan Caplan...|    1|       0|\n",
      "|Probing single el...|        0|Probing single el...|    1|       0|\n",
      "+--------------------+---------+--------------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode the label column 'frontpage' and show it to verify\n",
    "df = df.withColumn('frontpage', when(df.frontpage==True, 1).otherwise(0))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a06e5-0268-4249-9959-a99295c1e597",
   "metadata": {},
   "source": [
    "### Removing punctuations, stopwords, stemming and tokenizing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a823a-5177-4679-8886-993534a5520a",
   "metadata": {},
   "source": [
    "In preparation of featurization, the 'source_text' variable is tokenized so that sentences could be sliced into single words. After tokenization, punctuations and stop words have been removed. Then all the tokens are stemmed in terms of syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08884ed8-19a4-4099-ab92-a0fd90675fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text, remove the punctuations ('/\"/,/./:/-/?/!/:/|/[/])\n",
    "df_punc_drop = df.withColumn('source_text', regexp_replace(df.source_text, '[^a-zA-Z0-9]', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a3aa0c-3d10-4a37-9dfa-227f0018ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+--------+--------------------+\n",
      "|               title|frontpage|         source_text|votes|comments|              tokens|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+\n",
      "|A design for micr...|        0|Millimeter accura...|    1|       0|[millimeter, accu...|\n",
      "|Unreal Engine 5.4...|        0|Unreal Engine 5 4...|    1|       0|[unreal, engine, ...|\n",
      "|Sunlight and Vita...|        1|Sunlight and Vita...|   43|      42|[sunlight, and, v...|\n",
      "|Bryan Caplan on A...|        0|Q A  Bryan Caplan...|    1|       0|[q, a, , bryan, c...|\n",
      "|Probing single el...|        0|Probing single el...|    1|       0|[probing, single,...|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For text, make every word in lowercase\n",
    "df_token = Tokenizer(inputCol=\"source_text\", outputCol=\"tokens\").transform(df_punc_drop)\n",
    "df_token.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d95d27-b619-4659-a95e-245cf9ac0d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " \"he'll\",\n",
       " \"she'll\",\n",
       " \"we'll\",\n",
       " \"they'll\",\n",
       " \"i'd\",\n",
       " \"you'd\",\n",
       " \"he'd\",\n",
       " \"she'd\",\n",
       " \"we'd\",\n",
       " \"they'd\",\n",
       " \"i'm\",\n",
       " \"you're\",\n",
       " \"he's\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " \"we're\",\n",
       " \"they're\",\n",
       " \"i've\",\n",
       " \"we've\",\n",
       " \"you've\",\n",
       " \"they've\",\n",
       " \"isn't\",\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"mustn't\",\n",
       " \"can't\",\n",
       " \"couldn't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For text, remove stop words (a/an/the/then/and...)\n",
    "stopwords = StopWordsRemover()\n",
    "stopwords.getStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9605a2a-50f8-4e77-9366-d1062af8973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+\n",
      "|               title|frontpage|         source_text|votes|comments|              tokens|               words|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+\n",
      "|A design for micr...|        0|Millimeter accura...|    1|       0|[millimeter, accu...|[millimeter, accu...|\n",
      "|Unreal Engine 5.4...|        0|Unreal Engine 5 4...|    1|       0|[unreal, engine, ...|[unreal, engine, ...|\n",
      "|Sunlight and Vita...|        1|Sunlight and Vita...|   43|      42|[sunlight, and, v...|[sunlight, vitami...|\n",
      "|Bryan Caplan on A...|        0|Q A  Bryan Caplan...|    1|       0|[q, a, , bryan, c...|[q, , bryan, capl...|\n",
      "|Probing single el...|        0|Probing single el...|    1|       0|[probing, single,...|[probing, single,...|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.setInputCol('tokens').setOutputCol('words')\n",
    "df_clean = stopwords.transform(df_token)\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "615c4ed5-c94b-48ca-a2cc-20bc4e9934f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming with PorterStemmer\n",
    "def stem_words(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Register the UDF\n",
    "stem_udf = udf(stem_words, ArrayType(StringType()))\n",
    "\n",
    "# Apply the stemmer UDF\n",
    "df_final = df_clean.withColumn(\"words\", stem_udf(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "622dd256-caa2-4f4d-99bf-d9896ff96c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+\n",
      "|               title|frontpage|         source_text|votes|comments|              tokens|               words|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+\n",
      "|A design for micr...|        0|Millimeter accura...|    1|       0|[millimeter, accu...|[millimet, accur,...|\n",
      "|Unreal Engine 5.4...|        0|Unreal Engine 5 4...|    1|       0|[unreal, engine, ...|[unreal, engin, 5...|\n",
      "|Sunlight and Vita...|        1|Sunlight and Vita...|   43|      42|[sunlight, and, v...|[sunlight, vitami...|\n",
      "|Bryan Caplan on A...|        0|Q A  Bryan Caplan...|    1|       0|[q, a, , bryan, c...|[q, , bryan, capl...|\n",
      "|Probing single el...|        0|Probing single el...|    1|       0|[probing, single,...|[probe, singl, el...|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69db766-e4a8-4d1d-b089-34e7f06eb1be",
   "metadata": {},
   "source": [
    "### Featurization: method selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea03ea-dc7a-4d3b-a8eb-7dbc0e6db07c",
   "metadata": {},
   "source": [
    "After tokenization and stemming, now we have two methods of featurization: TF-IDF and Topic Modeling. They are chosen because in this text prediction course, we think certain keywords and topics are more engaging. So other methods like Word2Vec are not considered for they might be more useful in sentiment analysis instead of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d57f2f-4672-40d2-91ab-ac65eae8c9f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db098c05-7066-449c-a765-a015ade700ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"counts\")\n",
    "vectorizer_model = vectorizer.fit(df_final)\n",
    "df_topic = vectorizer_model.transform(df_final)\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5\n",
    "lda = LDA(k=num_topics, maxIter=10, featuresCol=\"counts\")\n",
    "lda_model = lda.fit(df_topic)\n",
    "training_new = lda_model.transform(df_topic)\n",
    "#Add columns for each topic\n",
    "for i in range(num_topics):\n",
    "    extract_topic_for_index_udf = udf(lambda topic_distribution, i=i: float(topic_distribution[i]), DoubleType())\n",
    "    training_new = training_new.withColumn(f\"topic_{i}\", extract_topic_for_index_udf(training_new[\"topicDistribution\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a1611-357d-42c3-bd02-4889664e50bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21139b9c-380d-4c21-a0ea-443ac4d35017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF-IDF\n",
    "# TF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "df_tf = hashingTF.transform(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57a0636c-6aaf-4d93-ae83-91596a703497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(df_tf)\n",
    "df_tf_idf = idfModel.transform(df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dff7bcbd-eb63-47d8-8c9b-ff1e6bd2d89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               title|frontpage|         source_text|votes|comments|              tokens|               words|         rawFeatures|            features|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|A design for micr...|        0|Millimeter accura...|    1|       0|[millimeter, accu...|[millimet, accur,...|(262144,[43,440,5...|(262144,[43,440,5...|\n",
      "|Unreal Engine 5.4...|        0|Unreal Engine 5 4...|    1|       0|[unreal, engine, ...|[unreal, engin, 5...|(262144,[22,30,49...|(262144,[22,30,49...|\n",
      "|Sunlight and Vita...|        1|Sunlight and Vita...|   43|      42|[sunlight, and, v...|[sunlight, vitami...|(262144,[43,156,1...|(262144,[43,156,1...|\n",
      "|Bryan Caplan on A...|        0|Q A  Bryan Caplan...|    1|       0|[q, a, , bryan, c...|[q, , bryan, capl...|(262144,[161,168,...|(262144,[161,168,...|\n",
      "|Medscape removes ...|        0|Medscape removes ...|    1|       0|[medscape, remove...|[medscap, remov, ...|(262144,[43,161,3...|(262144,[43,161,3...|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tf_idf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c311ce5b-3988-4b52-b548-aa315ac42717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "training_tfidf = normalizer.transform(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d044927e-7645-4904-9824-3f348043424e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o413.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 140.0 failed 1 times, most recent failure: Lost task 0.0 in stage 140.0 (TID 18083) (10.46.203.199 executor driver): java.lang.IllegalArgumentException: axpy only supports adding to a dense vector but got type class org.apache.spark.mllib.linalg.SparseVector.\r\n\tat org.apache.spark.mllib.linalg.BLAS$.axpy(BLAS.scala:76)\r\n\tat org.apache.spark.mllib.feature.PCA.$anonfun$fit$3(PCA.scala:52)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:62)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:487)\r\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\r\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\r\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.IllegalArgumentException: axpy only supports adding to a dense vector but got type class org.apache.spark.mllib.linalg.SparseVector.\r\n\tat org.apache.spark.mllib.linalg.BLAS$.axpy(BLAS.scala:76)\r\n\tat org.apache.spark.mllib.feature.PCA.$anonfun$fit$3(PCA.scala:52)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA \u001b[38;5;28;01mas\u001b[39;00m PCAml\n\u001b[0;32m      3\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCAml(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit(training_tfidf)\n\u001b[0;32m      5\u001b[0m training \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(training_tfidf)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o413.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 140.0 failed 1 times, most recent failure: Lost task 0.0 in stage 140.0 (TID 18083) (10.46.203.199 executor driver): java.lang.IllegalArgumentException: axpy only supports adding to a dense vector but got type class org.apache.spark.mllib.linalg.SparseVector.\r\n\tat org.apache.spark.mllib.linalg.BLAS$.axpy(BLAS.scala:76)\r\n\tat org.apache.spark.mllib.feature.PCA.$anonfun$fit$3(PCA.scala:52)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:62)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:487)\r\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\r\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\r\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.IllegalArgumentException: axpy only supports adding to a dense vector but got type class org.apache.spark.mllib.linalg.SparseVector.\r\n\tat org.apache.spark.mllib.linalg.BLAS$.axpy(BLAS.scala:76)\r\n\tat org.apache.spark.mllib.feature.PCA.$anonfun$fit$3(PCA.scala:52)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA as PCAml\n",
    "\n",
    "pca = PCAml(k=10, inputCol=\"normFeatures\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(training_tfidf)\n",
    "training = pca_model.transform(training_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de57695-9a9f-49f4-91fc-45f4ce1cd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8892df-863b-4bb6-a24a-b7e938b9491b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reduction on the dimensions (UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca73c1cd-cff2-4949-956d-b612a649d5a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIndexer, VectorAssembler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d9694-77e5-4d3c-911f-f787fcf95cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels\n",
    "indexer = StringIndexer(inputCol=\"frontpage\", outputCol=\"indexedLabel\")\n",
    "final_data = indexer.fit(rescaledData).transform(rescaledData)\n",
    "\n",
    "# Select the features and convert to a NumPy array\n",
    "features_array = np.array(assembled_data.select(\"features\").rdd.map(lambda row: row.features.toArray()).collect())\n",
    "\n",
    "# Apply UMAP\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_results = reducer.fit_transform(features_array)\n",
    "\n",
    "# Convert the UMAP results to a PySpark DataFrame\n",
    "umap_df = spark.createDataFrame(umap_results.tolist(), [\"UMAP1\", \"UMAP2\"])\n",
    "\n",
    "# Add the UMAP results to the original DataFrame\n",
    "final_umap_df = assembled_data.withColumn(\"id\", col(\"indexedLabel\")).join(umap_df.withColumn(\"id\", col(\"UMAP1\")), \"id\").drop(\"id\")\n",
    "\n",
    "# Show the results\n",
    "final_umap_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4468b-f839-4d9d-b24f-fd65eb871e95",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb166fa3-b489-476e-bd4e-fb75e62fff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=5, minCount=0, inputCol=\"words\", outputCol=\"features\")\n",
    "model = word2Vec.fit(df_final)\n",
    "df_wvec = model.transform(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c43911b-4b8c-4736-8997-15a225bd9573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+--------------------+\n",
      "|               title|frontpage|         source_text|votes|comments|              tokens|               words|            features|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+--------------------+\n",
      "|A design for micr...|        0|Millimeter accura...|    1|       0|[millimeter, accu...|[millimet, accur,...|[-0.1366724245900...|\n",
      "|Unreal Engine 5.4...|        0|Unreal Engine 5 4...|    1|       0|[unreal, engine, ...|[unreal, engin, 5...|[-0.0154509230286...|\n",
      "|Sunlight and Vita...|        1|Sunlight and Vita...|   43|      42|[sunlight, and, v...|[sunlight, vitami...|[-0.2339610227476...|\n",
      "|Bryan Caplan on A...|        0|Q A  Bryan Caplan...|    1|       0|[q, a, , bryan, c...|[q, , bryan, capl...|[-0.2994003929908...|\n",
      "|Probing single el...|        0|Probing single el...|    1|       0|[probing, single,...|[probe, singl, el...|[-0.1474760992471...|\n",
      "+--------------------+---------+--------------------+-----+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wvec.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaeb329-b39e-4643-a946-9b47968099f9",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e77226-3ea9-4b26-999a-37b1cf766145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "training = normalizer.transform(df_wvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2274c4-76c4-4f5e-9617-0ad388c0818a",
   "metadata": {},
   "source": [
    "### Train/test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb55cf55-2b38-4fed-a337-2ed09b771278",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split the data into train and test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m splits \u001b[38;5;241m=\u001b[39m df_wvec\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], \u001b[38;5;241m1234\u001b[39m)\n\u001b[0;32m      3\u001b[0m train \u001b[38;5;241m=\u001b[39m splits[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m splits[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:2091\u001b[0m, in \u001b[0;36mDataFrame.randomSplit\u001b[1;34m(self, weights, seed)\u001b[0m\n\u001b[0;32m   2085\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[0;32m   2086\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_NOT_POSITIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2087\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(w)},\n\u001b[0;32m   2088\u001b[0m         )\n\u001b[0;32m   2089\u001b[0m seed \u001b[38;5;241m=\u001b[39m seed \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, sys\u001b[38;5;241m.\u001b[39mmaxsize)\n\u001b[0;32m   2090\u001b[0m df_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mrandomSplit(\n\u001b[1;32m-> 2091\u001b[0m     _to_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, cast(List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m], weights)), \u001b[38;5;28mint\u001b[39m(seed)\n\u001b[0;32m   2092\u001b[0m )\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [DataFrame(df, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession) \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m df_array]\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\column.py:107\u001b[0m, in \u001b[0;36m_to_list\u001b[1;34m(sc, cols, converter)\u001b[0m\n\u001b[0;32m    105\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoList(cols)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[0;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(\n\u001b[0;32m   1713\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFLECTION_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   1714\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART)\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test\n",
    "splits = df_wvec.randomSplit([0.8, 0.2], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04cbbd-5c74-4d5e-830c-fed4b7ffe3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('title','tokens','words','source_text','votes','comments')\n",
    "test = test.drop('title','tokens','words','source_text','votes','comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1535a529-baf6-4174-b716-c9e7d2f691d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|frontpage|            features|\n",
      "+---------+--------------------+\n",
      "|        0|[-0.2649321264992...|\n",
      "|        0|[-0.2285036979353...|\n",
      "|        0|[-0.2318851084571...|\n",
      "|        0|[-0.1040695888514...|\n",
      "|        0|[-0.2843592267971...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eef1c-729c-4175-bb0d-57545691b24c",
   "metadata": {},
   "source": [
    "### Oversampling on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4413e-b7ee-4489-b295-d1b782fb7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of negative cases:', train.select('frontpage').where(train.frontpage==0).count())\n",
    "print('Count of positive cases:', train.select('frontpage').where(train.frontpage==1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2794d8-601e-4fdb-9340-4c3177947da1",
   "metadata": {},
   "source": [
    "Positive cases are much fewer that models could be unable to fully learn the patterns. To solve class imbalance and to consider the storage of Spark, undersampling is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e32f05-3d0b-45ee-90c2-062e82961648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create undersampling function\n",
    "def undersample_majority(df, ratio=1):\n",
    "    '''\n",
    "    ratio is the ratio of majority to minority\n",
    "    Eg. ratio 1 is equivalent to majority:minority = 1:1\n",
    "    ratio 5 is equivalent to majority:minority = 5:1\n",
    "    '''\n",
    "    minority_count = df.filter('frontpage')==1).count()\n",
    "    whole_count = df.count()\n",
    "    undersampled_majority = df.filter('frontpage'==0)\\\n",
    "                                .sample(withReplacement=False, fraction=(ratio*minority_count/whole_count),seed=42)\n",
    "    undersampled_df = df.filter('frontpage'==1).union(undersampled_majority)\n",
    "    \n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009967f-7ff1-4b18-9d20-d428f67a679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = undersampled_df_minority(train, ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62ea70-0c49-499c-a24f-cb94d268927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f43ee-79b5-4276-afdc-e095b79b27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of negative cases:', training_final.select('frontpage').where(training_final.frontpage==0).count())\n",
    "print('Count of positive cases:', training_final.select('frontpage').where(training_final.frontpage==1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1297289-7137-4214-8993-d6153c4b5b04",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b942954-8b85-4d36-bb58-f07e4bff9c9b",
   "metadata": {},
   "source": [
    "### Model 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01bff714-dbea-4894-8819-0b307f7b7cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50254)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\Admin\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\Admin\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"C:\\Users\\Admin\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m nbm \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mfit(train)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'No connection could be made because the target machine actively refused it', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_showtraceback(etype, value, stb)\n\u001b[0;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m gateway_client\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_cmd)\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol='features', labelCol='frontpage')\n",
    "\n",
    "# train the model\n",
    "nbm = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98257c-ebba-4442-a33b-2e2ab42560a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select example rows to display.\n",
    "predictions = nbm.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"frontpage\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48609c52-8ff4-4986-9915-e96a78ccaaa4",
   "metadata": {},
   "source": [
    "### Model 2: Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67ed6d54-fc57-40d9-8662-dcb2e5c39fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to the local file, please define own local directory here\n",
    "\n",
    "model_path = 'C:/Users/Admin/Advanced Analytics for Bid Data World/Assignment 3/models/naive_bayes'\n",
    "naive_bayes = nbm.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8adbdf4-60fe-4c58-8f90-7a3836c860dc",
   "metadata": {},
   "source": [
    "## Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ca762-0172-4033-b1ea-998c1e49dec7",
   "metadata": {},
   "source": [
    "There are two goals:\n",
    "(1) save the model;\n",
    "(2) preprocessing the incoming message.\n",
    "In the model deployment the preprocessing and tokenization steps are basically repeated to featurize the incoming stream data so that the model could understand the data. However, for all the tools we did not refit them in combination of the new data and the original data set. The new data is decomposited using the tools fitted on the original data set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bab3fa3-ef7c-457a-9779-e8f8a37e7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97ef99a8-bd92-4d5d-996d-fc224c1152ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a9e1409-98a9-4c50-800e-23861261dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb5be764-0de4-46d3-9e4d-657912ad5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "# Define the prediction function\n",
    "#def predict(df):\n",
    "    #return globals()['my_model'].transform(df)\n",
    "\n",
    "#predict_udf = udf(predict, FloatType())\n",
    "\n",
    "# The final function\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df_stream = spark.read.json(rdd)\n",
    "    #df_stream.show()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    df_punc_drop_stream = df_stream.withColumn('source_text', regexp_replace(df_stream.source_text, '[^a-zA-Z0-9]', ' '))\n",
    "    \n",
    "    # Transformed with tokens\n",
    "    df_token_stream = Tokenizer(inputCol=\"source_text\", outputCol=\"tokens\").transform(df_punc_drop_stream)\n",
    "    #df_token_stream.show()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    df_clean_stream = stopwords.transform(df_token_stream)\n",
    "    #df_clean_stream.show()\n",
    "    \n",
    "    # Apply Word2Vec\n",
    "    #result_stream = word_embed.transform(df_clean_stream)\n",
    "\n",
    "    # Apply TF-IDF\n",
    "    featurizedData_stream = hashingTF.transform(df_clean_stream)\n",
    "    rescaledData_stream = idfModel.transform(featurizedData_stream)\n",
    "    \n",
    "    # Finalized the training data\n",
    "    training_stream = rescaledData_stream.drop('source_text','tokens','words')\n",
    "    #training_stream.show()\n",
    "\n",
    "    # Make predictions with the selected model\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = NaiveBayesModel.load(model_path)\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model (uncomment below):\n",
    "    \n",
    "    df_result = globals()['my_model'].transform(training_stream)\n",
    "    df_result_output = df_result.drop('rawFeatures','words','tokens','features','rawPrediction')\n",
    "    df_result_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6db464-0cf1-4607-9243-98c60a8c811d",
   "metadata": {},
   "source": [
    "## Streaming prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89568df-6174-4bb8-ba78-65e209dccbb6",
   "metadata": {},
   "source": [
    "After defining the function, now the socket is connected. With each batch of data comming in, there is a prediction accordingly. Both probabilities and predicted classification are shown at the end of each entry. As seen from the result, most entries without appearing on the frontpage are predicted correctly. For some entries with appearing on the frontpage, the model could give a correct prediction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "914382e2-8e5a-48c4-b7cc-bc7ce388cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f706a4c-a545-4513-acb0-46fe490113e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49c4bfc6-7f04-46c4-8b63-9f26859584f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-22 15:02:10 =========\n",
      "+--------+--------+--------------+---------+-------------------+--------------------+--------------------+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|        domain|frontpage|          posted_at|        source_title|               title|                 url|       user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------+---------+-------------------+--------------------+--------------------+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "|40436229|       0|bisqwit.iki.fi|     true|2024-05-22 01:04:02|Rockman / Megaman...|Rockman / Mega Ma...|https://bisqwit.i...|JojoFatsani|    5|[-3751.5663977685...|[1.0,4.2246778939...|       0.0|\n",
      "+--------+--------+--------------+---------+-------------------+--------------------+--------------------+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:02:20 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|        source_title|               title|                 url|        user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "|40436230|       0|melodicambient.su...|    false|2024-05-22 01:04:04|Why Were Older Po...|Why Were Older Po...|https://melodicam...|      davikr|    1|[-34593.437951839...|           [1.0,0.0]|       0.0|\n",
      "|40436240|       0|      stcsoftware.us|    false|2024-05-22 01:05:11|The Missing Piece...|The Missing Piece...|https://www.stcso...|   kbrothers|    1|[-7697.8899730561...|[1.0,2.2372947215...|       0.0|\n",
      "|40436255|       0|         youtube.com|     true|2024-05-22 01:07:05|Introducing Inter...|Perplexity launch...|https://www.youtu...|shreyarajpal|    4|[-1377.0592647181...|[1.0,2.6507974949...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:02:30 =========\n",
      "+--------+--------+------------------+---------+-------------------+----------------+--------------------+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|            domain|frontpage|          posted_at|    source_title|               title|                 url|      user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+------------------+---------+-------------------+----------------+--------------------+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "|40436258|       0|        arpa-h.gov|    false|2024-05-22 01:07:22|         UPGRADE|Universal Patchin...|https://arpa-h.go...|      rntn|    1|[-12442.616746340...|[1.0,6.1473495496...|       0.0|\n",
      "|40436268|       0|     bloomberg.com|     true|2024-05-22 01:08:39|       Bloomberg|Humane Explores P...|https://www.bloom...|   marc__1|    4|[-1301.1225948729...|[1.0,3.6515338136...|       0.0|\n",
      "|40436269|       0|medium.com/rviragh|    false|2024-05-22 01:09:06|Just a moment...|Let's just go who...|https://medium.co...|logicallee|    1|[-566.87011991282...|[0.99993320355583...|       0.0|\n",
      "+--------+--------+------------------+---------+-------------------+----------------+--------------------+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:02:40 =========\n",
      "+--------+--------+-----------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|           domain|frontpage|          posted_at|        source_title|               title|                 url|        user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+-----------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "|40436275|       0|    pipedream.com|    false|2024-05-22 01:10:06|Creating an out-o...|Creating an out-o...|https://pipedream...|todsacerdoti|    1|[-31340.401670741...|           [1.0,0.0]|       0.0|\n",
      "|40436281|       0|thesimplewiki.com|    false|2024-05-22 01:11:19|Simple Wiki: Enga...|SimpleWiki – All ...|https://www.thesi...| twothreetwo|    2|[-650.77461001359...|[1.0,1.0867636177...|       0.0|\n",
      "|40436285|       0|          wsj.com|     true|2024-05-22 01:12:01|             wsj.com|China Is Winning ...|https://www.wsj.c...|    jseliger|    4|[-54.038513061682...|[0.08005873968000...|       1.0|\n",
      "|40436290|       0|     navilens.com|    false|2024-05-22 01:13:18|NaviLens EMPOWERI...|NaviLens Empoweri...|https://www.navil...| anotherevan|    1|[-37634.554767676...|           [1.0,0.0]|       0.0|\n",
      "|40436293|       0|      nytimes.com|    false|2024-05-22 01:13:40|A.I.’s Black Boxe...|A.I.'S Black Boxe...|https://www.nytim...|    nikikiki|    1|[-40492.612386389...|[1.26072948496777...|       1.0|\n",
      "+--------+--------+-----------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:02:50 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|        source_title|               title|                 url|          user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "|40436301|       0|twitter.com/wunde...|    false|2024-05-22 01:14:29|                   X|ChatGPT: Prompt I...|https://twitter.c...|wendythehacker|    3|[-664.17737877334...|[0.08005604942408...|       1.0|\n",
      "|40436304|       0|        memothon.com|    false|2024-05-22 01:14:51|Why do I have to ...|Why do I have to ...|https://blog.memo...|      memothon|    1|[-2724.5525798095...|[1.0,2.7472371097...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:03:00 =========\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+---------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|          domain|frontpage|          posted_at|        source_title|               title|                 url|     user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+---------+-----+--------------------+--------------------+----------+\n",
      "|40436309|       0|        phys.org|    false|2024-05-22 01:16:09|Tiger beetles fig...|Tiger beetles fig...|https://phys.org/...|PaulHoule|    1|[-107574.49656591...|           [0.0,1.0]|       1.0|\n",
      "|40436326|       0|digitalocean.com|    false|2024-05-22 01:18:55|Unlock Effortless...|Digital Ocean App...|https://www.digit...|     nikz|    2|[-20661.080765534...|           [1.0,0.0]|       0.0|\n",
      "|40436357|       4| theatlantic.com|     true|2024-05-22 01:23:20|OpenAI Just Gave ...|OpenAI just gave ...|https://www.theat...| lolinder|    8|[-24059.208106472...|[1.0,3.3399026721...|       0.0|\n",
      "+--------+--------+----------------+---------+-------------------+--------------------+--------------------+--------------------+---------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:03:10 =========\n",
      "+--------+--------+-------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|             domain|frontpage|          posted_at|        source_title|               title|                 url|          user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+-------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "|40436437|       0|    happyscribe.com|    false|2024-05-22 01:32:56|Happy Scribe: Aud...|Transcription and...|https://www.happy...|handfuloflight|    2|[-12687.027005989...|[1.0,4.7779350290...|       0.0|\n",
      "|40436469|       0|breakingdefense.com|     true|2024-05-22 01:38:33|The revolution th...|The revolution th...|https://breakingd...|    walterbell|    3|[-51038.434700088...|           [1.0,0.0]|       0.0|\n",
      "|40436499|       0|  sciencedirect.com|    false|2024-05-22 01:43:53|       ScienceDirect|Middle Palaeolith...|https://www.scien...|          rntn|    1|[-1632.3476035352...|[1.0,3.5464559473...|       0.0|\n",
      "|40436523|       0|            twz.com|    false|2024-05-22 01:46:51|U.S. Military Las...|U.S. Military Las...|https://www.twz.c...|        jdmark|    2|[-50353.842987900...|           [1.0,0.0]|       0.0|\n",
      "+--------+--------+-------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:03:20 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|        source_title|               title|                 url|       user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "|40436533|       0|             bbc.com|     true|2024-05-22 01:48:32|Microsoft Copilot...|UK watchdog looki...|https://www.bbc.c...|hassanahmad|    4|[-15077.709752779...|[1.0,2.9636394579...|       0.0|\n",
      "|40436560|       0|github.com/tillywork|     true|2024-05-22 01:54:24|GitHub - tillywor...|Tillywork: Open-S...|https://github.co...|thunderbong|    7|[-15040.160574400...|           [1.0,0.0]|       0.0|\n",
      "|40436564|       4|       unplugged.com|    false|2024-05-22 01:54:31|           Unplugged|Up Phone – The Pr...|https://unplugged...|     rmason|    6|[-19589.040726081...|[1.0,1.0880476445...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:03:30 =========\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|         domain|frontpage|          posted_at|        source_title|               title|                 url|        user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "|40436572|       0|   colab.google|    false|2024-05-22 01:55:24|Notebooks – colab...|   Curated Notebooks|https://colab.goo...|     jonbaer|    2|[-29047.965114747...|           [1.0,0.0]|       0.0|\n",
      "|40436605|       0|   uchicago.edu|    false|2024-05-22 02:02:22|Cinemetrics - Mov...|Movie Measurement...|https://cinemetri...|gibspaulding|    1|[-10401.353721690...|[1.0,5.8415673440...|       0.0|\n",
      "|40436607|       0|sciencenews.org|    false|2024-05-22 02:02:41|The neutrino’s qu...|The neutrino's qu...|https://www.scien...| thunderbong|    1|[-28588.177017917...|           [1.0,0.0]|       0.0|\n",
      "+--------+--------+---------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:03:40 =========\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ec9618f-9f78-438a-8ed9-cbf4a2d14d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|        source_title|               title|                 url|    user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------+-----+--------------------+--------------------+----------+\n",
      "|40436625|       0|twitter.com/apple...|     true|2024-05-22 02:06:29|                   X|Meta plans to not...|https://twitter.c...|    ad8e|    6|[-664.17737877334...|[0.08005604942408...|       1.0|\n",
      "|40436631|       0|        politico.com|    false|2024-05-22 02:06:54|    Just a moment...|Hemp and Marijuan...|https://www.polit...|DocFeind|    1|[-733.91380194780...|[0.84110203687607...|       0.0|\n",
      "|40436639|       0|            jmp.chat|    false|2024-05-22 02:07:43|Newsletter: SMS R...|JMP Newsletter: S...|https://blog.jmp....|    zaik|    1|[-10535.442378134...|[1.0,1.3567576113...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-22 15:03:50 =========\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "|     aid|comments|       domain|frontpage|          posted_at|        source_title|               title|                 url|          user|votes|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "|40436651|       0|karlstack.com|     true|2024-05-22 02:09:26|Exclusive: Scanda...|Scandal at Americ...|https://www.karls...|       potench|   21|[-32586.120576276...|           [1.0,0.0]|       0.0|\n",
      "|40436692|       0|    mitre.org|    false|2024-05-22 02:17:23|CVE - CVE-2024-32002|      CVE-2024-32002|https://cve.mitre...|gradientsrneat|    1|[-17790.721845848...|[1.0,1.8269818017...|       0.0|\n",
      "|40436693|       0|anthropic.com|    false|2024-05-22 02:17:34|Mapping the Mind ...|Anthropic: Mappin...|https://www.anthr...|    Davidzheng|    1|[-31537.745664828...|           [1.0,0.0]|       0.0|\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
